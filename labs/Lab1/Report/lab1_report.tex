\documentclass{article}
\usepackage{iclr2016_conference,times}

\usepackage{subfig}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{graphicx}
\bibliographystyle{abbrvnat}

\title{
	Mobile Computing (CS23400$/$1) \vspace{-4pt} \\
	{\Large Lab 1 - Report} \vspace{6pt} \\
	{\large Andrea F. Daniele $\hspace{2.2cm}$ Max X. Liu $\hspace{2.2cm}$ Noah A. Hirsch}
}

\begin{document}

\maketitle


\vspace{-1.2cm}

\section{Task}
\vspace{-.3cm}
The goal of this lab is to collect and leverage IMU data from a small robotic
car to classify a set of activities. Using the IMU data we can implement machine
learning models to classify IMU traces into four distinct activities (walking,
jumping, standing, driving). Furthermore, the classification model used should
be robust to challenges with low quality IMU's and mobile computation limits.

\section{Challenges}
\vspace{-.3cm}
One of the primary challenges faced was extracting consistent and relevant data
from the IMU traces. Due to imperfect measurements from the IMU, the raw data
contains a lot of noisy readings that complicate the problem of classifying
activities. Furthermore, it is very computationally intensive to train Recurrent
Neural Networks with such large trace files; additional pre-processing of data
is necessary to help train the model. An additional concern was that there may
not be enough data and/or features to effectively train a Recurrent Neural Network.

\section{Proposed Approach}
\vspace{-.3cm}
\textbf{Assigned to: Max}

The task naturally lended itself to a general data classification problem. We decided
to use a Recurrent Neural Network to solve this problem; specifically, we used LSTM
(Long Short Term Memory) networks. RNN's are extremely effective with this type of
trace data because it is able to take in sequences of data and make classifications
based on the context of both recent and past information. Thus, we can easily look at features
such as changes in x, y, z acceleration over some period of time. In order to quickly and effectively train
the RNN it is necessary to first do some data processing to reduce noise, decrease trace sizes, and
add additional features.


* Let's introduce the fact that we do some data post-processing before feeding it to the model
(the next section will be about data post-processing).

\subsection{Data post-processing}
\vspace{-.3cm}
\textbf{Assigned to: Andrea}

* Here we talk about the fact that we downsample the readings from $128Hz$ to $16Hz$. Let's
justify it by saying that we tried different factors and empirically chose the one that maximizes
reduction while preserving signal shape (we tried $2, 4, 8, 16, 32$).

* Let's also talk about the trimming phase, in which we discard the first and last $2$ seconds
of each trace because of the time spent to say ''Now you can start jumping``, or the time
required to switch windows (while Driving the car) and the time when somebody stops earlier and
asks ''Can I stop jumping?``.

* We also explain that since the signals are periodic (for all the activities that we have), we can
easily classify the activity based on a few seconds of data. Thus we split a $10$ seconds long
trace into $6$ smaller samples of $1$ second each (the $4$ seconds that are missing are removed
in the trimming phase above).


\subsection{Recurrent Neural Network}
\vspace{-.3cm}
\textbf{Assigned to: Andrea}

* Here we explain and show the architecture of our neural network by unwrapping the timesteps.

* Explain what is X, what is Y, and what is the hidden state. Do not talk about batching here.

* Explain (briefly) the training process and how gradient back-propagation works.


\subsection{Dataset}
\vspace{-.3cm}
\textbf{Assigned to: Andrea}

* Here we just talk about how much data (samples) we used for training, how much for validation
and how much for testing.

* Depending on whether we decide to use cross-validation we talk about it too.


\section{Results}
\vspace{-.3cm}
\textbf{Assigned to: Noah}

* Here we discuss and show a plot of the training/validation loss/accuracy over time (over epochs).

* We show the confusion matrix computed either on the validation set (if not cross-validation) or
on the training set (if cross-validation).


\section{Conclusion}
\vspace{-.3cm}
\textbf{Assigned to: Noah}

* Here we can talk about the possibility to deploy our model on mobile devices. Let's just say that
there exist an implementation for mobile devices of tensorflow so our approach can be easily and
seamlessly deployed on a mobile device.

\end{document}
